{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:56:09.125942Z",
     "iopub.status.busy": "2025-04-26T19:56:09.125698Z",
     "iopub.status.idle": "2025-04-26T19:56:19.459594Z",
     "shell.execute_reply": "2025-04-26T19:56:19.459035Z",
     "shell.execute_reply.started": "2025-04-26T19:56:09.125920Z"
    },
    "id": "fa65485e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "imports\n",
    "'''\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c587438f"
   },
   "source": [
    "## 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:56:19.460684Z",
     "iopub.status.busy": "2025-04-26T19:56:19.460316Z",
     "iopub.status.idle": "2025-04-26T19:56:19.685742Z",
     "shell.execute_reply": "2025-04-26T19:56:19.684880Z",
     "shell.execute_reply.started": "2025-04-26T19:56:19.460666Z"
    },
    "id": "1ef4903c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "load data into dataframe\n",
    "'''\n",
    "df = pd.read_csv(\"./data/Combined_News_DJIA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:56:19.688170Z",
     "iopub.status.busy": "2025-04-26T19:56:19.687923Z",
     "iopub.status.idle": "2025-04-26T19:56:19.704556Z",
     "shell.execute_reply": "2025-04-26T19:56:19.703832Z",
     "shell.execute_reply.started": "2025-04-26T19:56:19.688151Z"
    },
    "id": "32e48501",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "drop rows containing null values\n",
    "'''\n",
    "df[df.isnull().any(axis=1)]\n",
    "#can drop rows before melting, removing 9/15/2009, 12/24/2009,4/21/2011.\n",
    "df= df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:56:19.748935Z",
     "iopub.status.busy": "2025-04-26T19:56:19.748647Z",
     "iopub.status.idle": "2025-04-26T19:56:19.762302Z",
     "shell.execute_reply": "2025-04-26T19:56:19.761509Z",
     "shell.execute_reply.started": "2025-04-26T19:56:19.748909Z"
    },
    "id": "2e5e2d5a",
    "outputId": "0e92daf8-1de7-4eaf-da8f-611fdf83693f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sort dataframe by date, check distribution of up and down days. Will ensure even split in training dataframe.\n",
    "'''\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "#already sorted i believe, just in case.\n",
    "df = df.sort_values(by=\"Date\")\n",
    "print(f\"number of days price fell: {int((df['Label'] == 0).sum())}, number of days price rose {int((df['Label'] == 1).sum())}\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Clean and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:56:19.763277Z",
     "iopub.status.busy": "2025-04-26T19:56:19.763060Z",
     "iopub.status.idle": "2025-04-26T20:02:07.311149Z",
     "shell.execute_reply": "2025-04-26T20:02:07.310125Z",
     "shell.execute_reply.started": "2025-04-26T19:56:19.763254Z"
    },
    "id": "44eb6bff",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "load spacy, clean posts and split into tokens.\n",
    "'''\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_post(post):\n",
    "    '''\n",
    "    post:str\n",
    "    clean reminants of b\"\" assuming when data was scraped, it was binary str.\n",
    "    '''\n",
    "    #starts with ', \", b\", b', (capture this), end with ',\".\n",
    "    pattern = r\"^['\\\"]*b?['\\\"]?(.*?)['\\\"]?$\"\n",
    "    try:\n",
    "        match  = re.search(pattern,post,re.DOTALL)\n",
    "        if not match:\n",
    "            print(\"vital error\")\n",
    "        else:\n",
    "            #just convert to lowercase here, going to use spacy to tokenize\n",
    "            return match.group(1).strip().lower()\n",
    "    except Exception as e:\n",
    "        print(post)\n",
    "\n",
    "def tokenize(post):\n",
    "    '''\n",
    "    post:str \n",
    "    tokenizes given post. (spacy)\n",
    "    '''\n",
    "    #removing punctuation and white space, stop words may help with context.\n",
    "    tokens = [token.text for token in nlp(post) if not (token.is_space or token.is_punct or token.is_stop)] #\n",
    "    return tokens\n",
    "\n",
    "#clean and tokenize\n",
    "for i in range(1,26):\n",
    "    df[f\"Top{i}\"] = df[f\"Top{i}\"].apply(clean_post)\n",
    "    #tokenize\n",
    "    df[f\"Top{i}\"] = df[f\"Top{i}\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cfbf17b"
   },
   "source": [
    "### 1.3: Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(19)\n",
    "\n",
    "def balance_training_dataframe(train_df):\n",
    "    '''\n",
    "    train_df: pandas dataframe\n",
    "    balances data distribution\n",
    "    '''\n",
    "    copy = train_df.copy()\n",
    "\n",
    "    ones_indices = copy[copy[\"Label\"] == 1].index.tolist()\n",
    "\n",
    "    while (copy[\"Label\"] == 0).sum() != (copy[\"Label\"] == 1).sum():\n",
    "        if len(ones_indices) > 0:\n",
    "            rn = random.choice(ones_indices)\n",
    "            copy = copy.drop(rn)  \n",
    "            ones_indices.remove(rn)\n",
    "    print((copy[\"Label\"] == 0).sum(), (copy[\"Label\"] == 1).sum())\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.060Z",
     "iopub.execute_input": "2025-04-26T20:02:07.312483Z",
     "iopub.status.busy": "2025-04-26T20:02:07.312247Z"
    },
    "id": "1eec6abe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "set up test and train dataframe. set up vocabulary with tokens in training dataframe.\n",
    "'''\n",
    "\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df.iloc[0:train_size]\n",
    "test_df = df.iloc[train_size:]\n",
    "\n",
    "train_df = balance_training_dataframe(train_df)\n",
    "\n",
    "all_tokens = []\n",
    "unique_tokens = set()\n",
    "\n",
    "for _,row in train_df.iterrows():\n",
    "    for i in range(1,26):\n",
    "        col = row[f\"Top{i}\"]\n",
    "        for token in col:\n",
    "            all_tokens.append(token)\n",
    "            unique_tokens.add(token)\n",
    "\n",
    "vocab = sorted(list(unique_tokens))\n",
    "\n",
    "#remove any tokens from test not in train\n",
    "for i in range(1,26):\n",
    "    test_df.loc[:,f\"Top{i}\"] = test_df[f\"Top{i}\"].apply(lambda post: [token for token in post if token in vocab])\n",
    "\n",
    "#padding char is going to be zero, add 1 to idx\n",
    "encoder = {token:idx+1 for idx,token in enumerate(vocab)}\n",
    "encode = lambda post: [encoder[token] for token in post]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#no entries were completely removed\n",
    "cols = [f\"Top{i}\" for i in range(1,26)]\n",
    "len(test_df) == test_df[cols].map(lambda x : len(x) > 0 ).all(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max len of post is in our train data\n",
    "max_len_of_post = float(\"-inf\")\n",
    "for _,row in train_df.iterrows():\n",
    "    for i in range(1,26):\n",
    "        max_len_of_post = max(len(encode(row[f\"Top{i}\"])), max_len_of_post)\n",
    "max_len_of_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "541e871c"
   },
   "source": [
    "## 2: Visualization Of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.061Z"
    },
    "id": "cc3187a2",
    "outputId": "5b4688a1-ec50-40b3-db57-105eb21ba711",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "token_count = Counter(all_tokens)\n",
    "def plot_freq_k(k=20,stop=False):\n",
    "    '''\n",
    "    k: int \n",
    "    plots bar chart of top k tokens frequencies.\n",
    "    '''\n",
    "    most_common = token_count.most_common(k)\n",
    "    tokens,counts = zip(*most_common)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(tokens,counts)\n",
    "    plt.title(f\"Top {k} Most Common Tokens\")\n",
    "    plt.show()\n",
    "\n",
    "plot_freq_k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_distribution():\n",
    "    '''\n",
    "    plots dist of tokens\n",
    "    '''\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.hist(list(token_count.values()), bins=100)\n",
    "    #was way too big cuz of bin\n",
    "    plt.xlim(0,500)\n",
    "    plt.xlabel(\"Number of Tokens\")\n",
    "    plt.ylabel(\"Token Frequency\")\n",
    "    \n",
    "    plt.title(\"Token Frequency Distribution\")\n",
    "    plt.show()\n",
    "plot_token_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8de03940"
   },
   "source": [
    "## 3: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.061Z"
    },
    "id": "4de2d898",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#****************************#\n",
    "#******hyper parameters******#\n",
    "embed_size = 128\n",
    "batch_size = 64 \n",
    "block_size = max_len_of_post\n",
    "learning_rate = 1e-5\n",
    "n_heads,n_layers = 8, 4\n",
    "vocab_size = len(vocab)\n",
    "epochs = 170\n",
    "#******hyper parameters******#\n",
    "#****************************#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Tensor Creation & Batch Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.061Z"
    },
    "id": "40ace85a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def handle_padding(post):\n",
    "    '''\n",
    "    post:str\n",
    "    adds padding tokens to each post.\n",
    "    '''\n",
    "    post = encode(post)\n",
    "    \n",
    "    #add padding char (0)\n",
    "    if len(post) < max_len_of_post:\n",
    "        return post + [0] * (max_len_of_post -len(post))\n",
    "    else:\n",
    "        return post\n",
    "\n",
    "def get_batch(batch,df=train_df):\n",
    "    '''\n",
    "    batch_num:int [0, len(df) // batch_size]\n",
    "    df:dataframe\n",
    "    gets batch B,P=25,T\n",
    "    '''\n",
    "    cols = [f\"Top{i}\" for i in range(1,26)]\n",
    "    x = []\n",
    "    y = []\n",
    "    #inputs\n",
    "    batch = df.iloc[batch:batch+batch_size]\n",
    "  \n",
    "    for _,row in batch.iterrows():\n",
    "        #encodes and adds padding token(0)\n",
    "        x.append([handle_padding(post) for post in row[cols].to_list()])\n",
    "        y.append(row[\"Label\"])\n",
    "\n",
    "    x = torch.tensor(x).to(device)\n",
    "    y = torch.tensor(y).to(device).unsqueeze(1)\n",
    "    #padding mask\n",
    "    padding_mask  = (x !=0).byte()\n",
    "    return padding_mask,x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.061Z"
    },
    "id": "aca33040",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Wanted to cite Andrej Kaparthy's lecture as it essentially walked me through the code and concepts.\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3423s \n",
    "https://github.com/karpathy/ng-video-lecture\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "maybe add dropouts\n",
    "'''\n",
    "#for reproduction\n",
    "torch.manual_seed(19)\n",
    "\n",
    "class WeightedSumTokenPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        w: fully connected layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(embed_size,1) #kind of shallow\n",
    "    def forward(self,x,padding_mask):\n",
    "        attn = self.w(x) # B,P,T,1\n",
    "        padding_mask = padding_mask.unsqueeze(3) # B,P,T,1\n",
    "        attn = attn.masked_fill(padding_mask == 0, float(\"-inf\")) # get rid of padding\n",
    "        attn = F.softmax(attn,dim=2) # calc weights\n",
    "        x = torch.sum(x * attn,dim=2) #weights broadcast horizontally scaling tokens\n",
    "        return x \n",
    "\n",
    "\n",
    "class WeightedSumPostPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        w: fully connected layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(embed_size,1)\n",
    "    def forward(self,x):\n",
    "        attn = self.w(x)# B,P,1\n",
    "        attn = F.softmax(attn,dim=1)\n",
    "        x = torch.sum(x * attn,dim=1) #weights broadcast horizontally scaling posts\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        seq : fully connected layer\n",
    "        '''\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(embed_size,embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size,embed_size)\n",
    "        )\n",
    "    #x is B,P,T,C\n",
    "    def forward(self,x):\n",
    "        out = self.seq(x)\n",
    "        return out\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    '''\n",
    "    the bread and butter...\n",
    "    query: linear (embed => head), what each token is looking for\n",
    "    key: linear(embed => head), what each token has\n",
    "    value: linear(embed => head), the actual contexualized value of each token\n",
    "    '''\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_size,head_size,bias=False)\n",
    "        self.key = nn.Linear(embed_size,head_size,bias=False)\n",
    "        self.value = nn.Linear(embed_size,head_size,bias=False)\n",
    "    #x is B,P,T,C\n",
    "    def forward(self,x,padding_mask):\n",
    "        \n",
    "        #linear pass\n",
    "        #B,P,T,C => B,P,T,head_size\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        #attention scores\n",
    "        #(kinda cosine similiarty with different scaling?),\n",
    "        #B,P,T,head_size @ B,P,head_size,T => B,P,T,T\n",
    "        attn = (q @ k.transpose(-2,-1)) * (k.shape[-1] ** -0.5)\n",
    "\n",
    "        #***************************#\n",
    "        #**casual mask implemented**#\n",
    "        #***************************#\n",
    "\n",
    "        attn = torch.tril(attn)\n",
    "        attn = attn.masked_fill(attn ==0, float(\"-inf\"))\n",
    "        \n",
    "        '''\n",
    "        thinking: x is of shape B,P,T,T. padding mask is B,P,T.\n",
    "        unsqueezing at dim 2 will make padding mask to be B,P,1,T. so we have a row representation of mask. (essentially we pad out the attn scores involved with padding tokens not the actual padding tokens.)\n",
    "        let x = real y = fake.\n",
    "        [\n",
    "            y y y y      y x x       y y y\n",
    "            x x x x  @   y x x    =  y x x \n",
    "            x x x x      y x x       y x x  \n",
    "        ]                y x x\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        col_mask = padding_mask.unsqueeze(2) #should broadcast\n",
    "        attn = attn.masked_fill(col_mask == 0, float(\"-inf\"))\n",
    "\n",
    "        #prob\n",
    "        attn = F.softmax(attn,dim=-1)\n",
    "\n",
    "        #feed information into value vectors\n",
    "        out = attn @v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    head_size: replaces C dimensions, need trainable weights so C => head_size is q,k,v matrices.\n",
    "    heads: module list so diff parameters for each instance\n",
    "    lin: combine everything each head learned then linear layer to combine it as a single representation.\n",
    "    '''\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(n_heads))\n",
    "        self.lin = nn.Linear(head_size * n_heads,embed_size)\n",
    "    #x is B,P,T,C\n",
    "    def forward(self,x,padding_mask):\n",
    "        #concat head's on last dimension\n",
    "        out = torch.cat([h(x,padding_mask) for h in self.heads],dim=-1)\n",
    "        out  = self.lin(out)\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''\n",
    "    sa_heads: self attention heads\n",
    "    feed_forward: fully connected layer\n",
    "    l1,l2:layer norms\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.sa_heads =  MultiHeadAttention(head_size)\n",
    "        \n",
    "        self.feed_forward= FeedForward()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "    def forward(self,x,padding_mask):\n",
    "        #add residuals, tokens are going to change, but since it may be better to keep some of the old information we add it as a residual.\n",
    "        x = self.ln1(x + self.sa_heads(x,padding_mask))\n",
    "        out = self.ln2(x + self.feed_forward(x))\n",
    "        return out\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    '''\n",
    "    token_embedding: embed for each token in vocab\n",
    "    position_embedding: embed for index of token in range [0-max_len_of_post]\n",
    "    ln1,ln2: layer norm\n",
    "    t_pool: weighted sum pooling across T\n",
    "    p_pool: weighted sum pooling across P\n",
    "    blocks: a simplified block. the amount of actual mha passes were going to have (n_layers)\n",
    "    classifier: embed size -> 1 for classification\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size+1,embed_size) # add one for padding\n",
    "        self.position_embedding = nn.Embedding(block_size,embed_size)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.t_pool = WeightedSumTokenPooling()\n",
    "\n",
    "        '''\n",
    "        important! remove this if loading WeightedSumT_MeanP\n",
    "        ∇ ∇ ∇ ∇\n",
    "        '''\n",
    "        self.p_pool = WeightedSumPostPooling()\n",
    "\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layers)])\n",
    "\n",
    "        self.classifier = nn.Linear(embed_size,1)\n",
    "\n",
    "    def forward(self,padding_mask,x,labels=None):\n",
    "        T = x.shape[2]\n",
    "\n",
    "        #embeddings\n",
    "        x = self.token_embedding(x)\n",
    "        x = x + self.position_embedding(torch.arange(T,device=device))\n",
    "\n",
    "\n",
    "        #layers\n",
    "        for block in self.blocks:\n",
    "            x = block(x,padding_mask)\n",
    "\n",
    "        '''\n",
    "        My thought process here. at this point we have a B,P,T,C which we need to dial down to two dimensions. I attempted doing mean pooling for the tokens after mha pass,\n",
    "        which I believe constricted the amount of information in the sense that it did not weigh the actual tokens importance. ex: \"the\" had the same importance of \"bearish\".\n",
    "\n",
    "        For the post & token dimension, I think a combo of both weighted sum and (mean or max) pooling is the way to go since the model may be able to find certain tokens/posts that were more important \n",
    "        based off the actual context thanks to the mha layer. Better than taking the avg or max across T,P dimensions.\n",
    "        '''\n",
    "     \n",
    "        #pool tokens\n",
    "        x = self.t_pool(x,padding_mask) #B,P,T,C => B,P,C\n",
    "\n",
    "        x = self.ln1(x) #maybe\n",
    "\n",
    "        '''\n",
    "        important! must change to mean if loading WeightedSumT_MeanP\n",
    "        x = x.mean(dim=1)\n",
    "        ∇ ∇ ∇ ∇\n",
    "        '''\n",
    "        #pool posts\n",
    "        x = self.p_pool(x) #B,P,C => B,C \n",
    "\n",
    "        x = self.ln2(x) #maybe\n",
    "   \n",
    "        #logits\n",
    "        logits = self.classifier(x) # B,C => B,1\n",
    "\n",
    "        if labels == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #bce instead of ce since there are only two classes\n",
    "            loss = F.binary_cross_entropy_with_logits(logits,labels.float())\n",
    "        return logits,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.061Z"
    },
    "id": "a0080f89",
    "outputId": "5500490f-3e1d-4818-d80a-2d330c48486d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m = SentimentModel()\n",
    "m = m.to(device)\n",
    "#print total params\n",
    "print(f\"number of params: {sum(p.numel() for p in m.parameters())}\")\n",
    "optimizer = torch.optim.AdamW(m.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1: Helper Functions For Visualizing/Understanding Model's Progression/Predictive Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visfp = \"./visualization/\"\n",
    "\n",
    "def visualize_metrics_trend(accuracy,precision,recall,f1,losses):\n",
    "    '''\n",
    "    accuracy: per epoch\n",
    "    precision: per epoch\n",
    "    recall: per epoch\n",
    "    f1: per epoch\n",
    "    losses: per epoch\n",
    "    line plot visualize training metrics for training over epochs\n",
    "    '''\n",
    "    plt.plot(accuracy, label=\"Accuracy\")\n",
    "    plt.plot(precision, label=\"Precision\")\n",
    "    plt.plot(recall, label=\"Recall\")\n",
    "    plt.plot(f1, label=\"F1\")\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Train Metrics over epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def handle_metrics(labels,preds):\n",
    "    '''\n",
    "    labels: correct labels\n",
    "    preds: predictions\n",
    "    calcuates metrics accuracy, precision, recall, and f1\n",
    "    '''\n",
    "    accuracy = accuracy_score(labels,preds)\n",
    "    precision = precision_score(labels,preds,zero_division=0)\n",
    "    recall = recall_score(labels,preds,zero_division=0)\n",
    "    f1 = f1_score(labels,preds,zero_division=0)\n",
    "    print(f\"accuracy: {accuracy}, precision: {precision}, recall: {recall}, f1: {f1}\")\n",
    "    return accuracy,precision,recall,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b12fab0"
   },
   "source": [
    "#### 3.4.2: Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.061Z"
    },
    "id": "0c1fadd2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fp = \"./tensors/\"\n",
    "def train():\n",
    "    m.train()\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall =[]\n",
    "    f1 = []\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        preds = []\n",
    "        labels = []\n",
    "        #each row is a batch\n",
    "        for i in range((len(train_df) // batch_size)):\n",
    "            #forward pass\n",
    "            batch = i * batch_size\n",
    "            padding_mask,x,y = get_batch(batch,df=train_df)\n",
    "            logits,loss = m(padding_mask,x,y)\n",
    "            #manually calc predictions\n",
    "            p = F.sigmoid(logits)\n",
    "            p = (p>=0.5).int() \n",
    "            p = p.squeeze(1).cpu().tolist()\n",
    "            y = y.squeeze(1).cpu().tolist()\n",
    "            #save predictions and labels\n",
    "            preds.extend(p)\n",
    "            labels.extend(y)\n",
    "            #step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #calc avg loss\n",
    "            avg_loss += loss.detach()\n",
    "        avg_loss = avg_loss / (len(train_df) // batch_size)\n",
    "        print(f\"epoch: {epoch}, avg_loss: {avg_loss}\")\n",
    "        #met = accuracy,precision,recall,f1\n",
    "        met = handle_metrics(labels,preds)\n",
    "        losses.append(avg_loss.cpu())\n",
    "        #prepare arrays for visualization\n",
    "        accuracy.append(met[0])\n",
    "        precision.append(met[1])\n",
    "        recall.append(met[2])\n",
    "        f1.append(met[3])\n",
    "        if avg_loss < 0.3:\n",
    "          torch.save({\n",
    "              \"epoch\": epoch,\n",
    "              \"model_state_dict\":m,\n",
    "              \"optimizer_state_dict\":optimizer.state_dict(),\n",
    "          },fp + \"WeightedSumT_MeanP.pth\")\n",
    "          visualize_metrics_trend(accuracy,precision,recall,f1,losses)\n",
    "          return\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.062Z"
    },
    "id": "55ad9a12",
    "outputId": "a1760f92-8009-4423-bc91-95accaa66a08",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#train()\n",
    "###\n",
    "# training will take a really long time without gpu. I saved two model states:\n",
    "#    -   one with weighted sum pooling on both T and P. 'WeightedSumTP.pth'\n",
    "#    -   weighted sum pooling on both T and mean on P. 'WeightedSumT_MeanP.pth'\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Visualize Model's Result Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metric_score(met):\n",
    "    '''\n",
    "    met: metric scores\n",
    "    bar plot for visualizing test metrics\n",
    "    '''\n",
    "    metrics = [\"Accuracy\",\"Precision\",\"Recall\",\"F1\"]\n",
    "    plt.bar(metrics,met)\n",
    "    plt.title(\"Test Metrics\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2: Testing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.062Z"
    },
    "id": "ce92bd34",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint  = torch.load(\"./tensors/WeightedSumTP.pth\",weights_only=False,map_location=device) #or WeightedSumT_MeanP.pth, make sure to change model. I left comments on what to change.\n",
    "m = checkpoint[\"model_state_dict\"]\n",
    "print(f\"training epoch: {checkpoint[\"epoch\"]}\")\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "  m.eval()\n",
    "  preds = []\n",
    "  labels = []\n",
    "  avg_loss = 0\n",
    "  for i in range((len(test_df) // batch_size)):\n",
    "    batch = i * batch_size\n",
    "    padding_mask,x,y = get_batch(batch,df=test_df)\n",
    "    logits ,loss = m(padding_mask,x,y)\n",
    "    #manually calculate preds\n",
    "    p = F.sigmoid(logits)\n",
    "    p = (p>=0.5).int() \n",
    "    #save preds and labels\n",
    "    p = p.squeeze(1).cpu().tolist()\n",
    "    y = y.squeeze(1).cpu().tolist()\n",
    "    preds.extend(p)\n",
    "    labels.extend(y)\n",
    "    avg_loss += loss.detach()\n",
    "    print(f\"batch : {i}\")\n",
    "  avg_loss = avg_loss / (len(train_df) // batch_size)\n",
    "  print(f\"avg_loss: {avg_loss}\")\n",
    "  #visualize metrics\n",
    "  met = handle_metrics(labels,preds)\n",
    "  visualize_metric_score(met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-26T20:08:02.062Z"
    },
    "id": "79630bd2",
    "outputId": "5f42b633-ad1e-4de1-807b-579817d43b8f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Deallocate Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 129,
     "sourceId": 792900,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
